---
title: 'IEEE TNNLS Special Issue Call for Papers: Advancements in Foundation Models'
author: Cheng Jin
layout: post
group: news
cover: /static/img/news/2024_tnnls.jpg
oneline_description: 'Call for Papers: Special Issue on Advancements in Foundation Models for IEEE TNNLS.'
last_updated: 2024/05/30
---

<img src="/static/img/news/2024_tnnls.jpg" alt="IEEE TNNLS Special Issue" class="img-fluid">

The IEEE Transactions on Neural Networks and Learning Systems (TNNLS) invites submissions for a special issue on "Advancements in Foundation Models."

## Introduction

The rapid advancements in the domains of large language models (LLMs), large vision models
(LVMs), and large multimodal foundation models (LMMs), have opened up new possibilities for
transforming many aspects of scientific research, engineering, healthcare, agriculture,
education, and beyond. The applications of these foundation models in many domains have
yielded encouraging performance in various types of downstream tasks. However, further
research is needed to improve and validate the effectiveness, robustness, and generalizability of
foundation models across diverse scenarios. In response, this special issue aims to explore
cutting-edge research and development of foundation models and their applications in various
domains. We invite original research articles, systematic reviews, and domain-specific studies
that showcase the latest breakthroughs, challenges, and future directions in this emerging field
of foundation models. We welcome submissions from scholars working at the intersection of
artificial intelligence, machine learning, and domain applications. The special issue aims to
foster cross-disciplinary collaboration and promote the development of innovative, ethically
sound, and domain-specific solutions that can positively impact humanity and our society.
Submissions should adhere to rigorous scientific standards, including clear problem
formulation, reproducible methodology description, thorough evaluation, and meaningful
interpretation of results in the context of existing literature and research practice.

## Topics

Topics of interest include, but are not limited to:

- New neural network architecture design for foundation models.
- New training, fine-tuning, and adaptation methods for foundation model development.
- New multimodality alignment algorithms for foundation models.
- New paradigm for incorporating human feedback for aligning foundation models.
- New methods for optimizing, accelerating, and compressing foundation models and their
  applications.
- New frameworks for fusing and integrating multiple foundation models and their
  applications.
- Privacy-preserving training of foundation models their applications in specialized
  domains, such as healthcare and finance.
- Interpretability and explainability of foundation models.
- Trustworthiness of foundation models, such as ethical considerations and bias
  mitigation.
- New methods for mitigating hallucinations in the data generated by foundation models.
- Novel applications of foundation models in healthcare, science, engineering, agriculture,
  education, business, arts, humanities, and beyond.
- Evaluation metrics, open datasets, and benchmarking for foundation models.

## Dates:

Manuscript submission: August 15th, 2024

Preliminary decision: October 15th, 2024

Revisions due: January 1st, 2025

Final decision: February 15th, 2025

## Guest Editors:

Tianming Liu, University of Georgia, USA

Xiang Li, Massachusetts General Hospital and Harvard Medical School, USA

Hao Chen, Hong Kong University of Science and Technology, Hong Kong, China

Yixuan Yuan, Chinese University of Hong Kong, Hong Kong, China

Anirban Mukhopadhyay, TU Darmstadt, Germany
